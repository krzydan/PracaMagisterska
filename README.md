# PracaMagisterska

This is the code used to write a master's degree thesis titled **"Classifying Music Genres Using Machine Learning"**. Author: Krzysztof Danielak. 

The project was based on Keras, Tensorflow and a few machine learning algorithms from scikit-learn.

Code is shared under license [GPL v3](https://www.gnu.org/licenses/gpl-3.0.html)

## Source datasets

There were used two datasets of music.

- [GTZAN](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification) The dataset consists of 1000 audio tracks each 30 seconds long. It contains 10 genres, each represented by 100 tracks.The tracks are all 22050Hz Mono 16-bit audio files in .wav format.
-  [ISMIR2004](https://zenodo.org/record/1302992#.W18l9hixXMU)The audio is in MP3 format. It is divided into three folders, representing different subsets of the collection. Each folder has 729 files, split into classes. The number of files in each category reflects the proportion of files in each category in Magnatune when the dataset was created. No track appears in more than one folder.

    Training: files for generating a classification model, arranged by class.

    Development: A separate set of files for participants to test their model against.

    Evaluation: originally a private subset, the files used to evaluate the accuracy of all submitted models

The training and development set each consist of:

    classical: 320 files

    electronic: 115 files

    jazz_blues: 26 files

    metal_punk: 45 files

    rock_pop: 101 files

    world: 122 files
For the purposes of the thesis only training set was used.

## Pre-requirements

    Python >= 3.6

    pip >= 19.0.3

You have also install packages from requirements.txt.

## Running

You should run the code by calling this:
```
__main__.py <dataset> --clf <classifier> [classifier parameters] [-r <music representation parameter type> -v <value of that parameter>]
```
where:
- dataset is dataset path where is the audio. In the case of generation mel spectrograms or models this is the root folder where the files will be saved.
- classifier is type of classic machine learning algorithm or convolutional neural network architecture. Each classifier has its own parameters that can be specified. Available classifiers are:
1. *knn* - K-nearest neighbours. Possible parameters are (possible values are the same as in scikit-learn) ***--weights*** (weight function which is 'uniform' or 'distance'), **--n_neighbors** (number of neighbours which is integer), **--metric** (which is distance metric for the tree),  --pca (True or False. If True the model will be using PCA algorithm. Optional parameter, the default value is False. If True, you may specify numer of components to keep by using **--n_components** option.),**--repeats** (how many times repreat experiment) and **--duration** (how long frament of audio files to use in seconds). If '-r' option not specified then you can set -v. This is a positive integer that sets how many times artificially multiply set size (v times randomly chosen 2-second framgents from each audio)
2. *svm* - support-vector machine. You can specify those parameters (values the same as in scikit-learn). **--kernel** (the type of the kernel used by algorithm), **--degree** (degree of the polynomial kernel function), **--cfun** (regularization parameter which must be postive), **--pca** (True or False. If True the model will be using PCA algorithm. Optional parameter, the default value is False If True, you may specify numer of components to keep by using --n_components option.), **--repeats** (how many times repreat experiment) and **--duration** (how long frament of audio files to use in seconds). If '-r' option not specified then you can set -v. This is a positive integer that sets how many times artificially multiply set size (v times randomly chosen 2-second framgents from each audio)
3.  *cnn_3conv*, *cnn_4conv*,*cnn_4convnodropout* are convolutional neural networks that have accordingly 3 or 4 sets of convolutional, max pool and dropout layers. cnn_4convnodropout doesn't have dropout layers. Possible parameters are **--minibatch** (size of the batch - number of examples in a batch) and **--epochs** (number of epochs)
4.  *vgg16_fine*, *vgg16_transfer* are convolutional neural networks based on vgg16 architecture. The first one uses fine tuning for fitting. The second one uses transfer learning. Possible options are the same as for the rest of the neural networks.
- *-r* or *--representation* is optional parameter which is representation. It indicates the representation of the audio files. It can be **mp3** (lets use ISMIR set; no -v needed), **MFFCs** (using melcepstral coeffitients which is the default value). **images_librosa** and **images_plt** use mel spectrograms as datasets generated by using librosa or pyplot library.
 
Generating mel spectrograms is done by using just by setting -r option as *generatelibrosa* or *generateplt* which defines what library will be used for generations. No need for other options than path for saving spectrograms, for example:

```
__main__.py path -r generateplt
```

Every experiment returns mean and stadnard deviation (after 10 or 3 runs) of each of the scores which are 10 fold cross-validation accuracy, F-score, AUC and Cohen's kappa.

More details and results of each experiment you can find in the thesis (written in Polish).
